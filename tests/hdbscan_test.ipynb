{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import logging\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('hdbscan_clustering')\n",
    "\n",
    "def get_container_aisles(container_id: str, container_data: pd.DataFrame, \n",
    "                         sku_aisle_mapping: Dict[str, List[int]]) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Get optimized aisles required for a specific container, minimizing total aisles visited.\n",
    "    Chooses the best aisle for multi-location SKUs by considering other SKUs in the container.\n",
    "    \"\"\"\n",
    "    # Get all SKUs for this container\n",
    "    container_skus = container_data[container_data['container_id'] == container_id]['item_number'].unique()\n",
    "\n",
    "    # First, identify single-location SKUs - these must be visited\n",
    "    must_visit_aisles = set()\n",
    "    multi_location_skus = []\n",
    "\n",
    "    for sku in container_skus:\n",
    "        if sku in sku_aisle_mapping:\n",
    "            aisles = sku_aisle_mapping[sku]\n",
    "            if len(aisles) == 1:\n",
    "                # Single location SKU - must visit this aisle\n",
    "                must_visit_aisles.add(aisles[0])\n",
    "            else:\n",
    "                # Multi-location SKU - will optimize later\n",
    "                multi_location_skus.append(sku)\n",
    "\n",
    "    # For multi-location SKUs, choose aisles to minimize additional aisles\n",
    "    for sku in multi_location_skus:\n",
    "        aisles = sku_aisle_mapping[sku]\n",
    "        \n",
    "        # Check if any of the SKU's aisles are already in the must-visit set\n",
    "        already_covered = [aisle for aisle in aisles if aisle in must_visit_aisles]\n",
    "        \n",
    "        if already_covered:\n",
    "            # If one or more aisles are already covered, pick the first one\n",
    "            best_aisle = already_covered[0]\n",
    "        else:\n",
    "            # Otherwise, find the aisle that minimizes the distance to the nearest must-visit aisle\n",
    "            # If no must-visit aisles yet, choose the first available aisle\n",
    "            if not must_visit_aisles:\n",
    "                best_aisle = aisles[0]\n",
    "            else:\n",
    "                # Calculate \"distance\" to the nearest must-visit aisle for each option\n",
    "                min_distance = float('inf')\n",
    "                best_aisle = None\n",
    "                \n",
    "                for aisle in aisles:\n",
    "                    # Find distance to closest must-visit aisle\n",
    "                    closest_distance = min(abs(aisle - existing) for existing in must_visit_aisles)\n",
    "                    \n",
    "                    if closest_distance < min_distance:\n",
    "                        min_distance = closest_distance\n",
    "                        best_aisle = aisle\n",
    "        \n",
    "        # Add the best aisle to the must-visit set\n",
    "        must_visit_aisles.add(best_aisle)\n",
    "\n",
    "    return must_visit_aisles\n",
    "\n",
    "def compute_container_features(container_id: str, container_data: pd.DataFrame, \n",
    "                              sku_aisle_mapping: Dict[str, List[int]]) -> Tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    Compute feature vector for a container: (aisle_centroid, aisle_span, distinct_aisles)\n",
    "    \"\"\"\n",
    "    # Get container aisles\n",
    "    aisles = get_container_aisles(container_id, container_data, sku_aisle_mapping)\n",
    "    \n",
    "    if not aisles:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    # Calculate aisle centroid and span\n",
    "    centroid = sum(aisles) / len(aisles)\n",
    "    span = max(aisles) - min(aisles) if len(aisles) > 1 else 0\n",
    "    \n",
    "    # Count distinct aisles\n",
    "    distinct_aisles = len(aisles)\n",
    "    \n",
    "    return centroid, span, distinct_aisles\n",
    "\n",
    "def preprocess_data(container_data: pd.DataFrame, slotbook_data: pd.DataFrame, \n",
    "                   use_distinct_aisles: bool = True,\n",
    "                   centroid_weight: float = 0.5,\n",
    "                   secondary_weight: float = 0.5) -> Tuple[List[str], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Preprocess container data to create feature matrix for clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    container_data : pd.DataFrame\n",
    "        DataFrame containing container data with container_id and item_number\n",
    "    slotbook_data : pd.DataFrame\n",
    "        DataFrame containing slotbook data with item_number and aisle_sequence\n",
    "    use_distinct_aisles : bool\n",
    "        Whether to use distinct aisles count (True) or aisle span (False) as second feature\n",
    "    centroid_weight : float\n",
    "        Weight for the centroid feature\n",
    "    secondary_weight : float\n",
    "        Weight for the secondary feature (distinct aisles or span)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    valid_containers : List[str]\n",
    "        List of container IDs that were successfully processed\n",
    "    feature_matrix : np.ndarray\n",
    "        Matrix of normalized features for each container\n",
    "    raw_feature_matrix : np.ndarray\n",
    "        Matrix of raw (unweighted, unnormalized) features for each container\n",
    "    \"\"\"\n",
    "    logger.info(\"Preprocessing container data for clustering\")\n",
    "    \n",
    "    # Build SKU-to-aisle mapping\n",
    "    sku_aisle_mapping = {}\n",
    "    for _, row in slotbook_data.iterrows():\n",
    "        if row['item_number'] not in sku_aisle_mapping:\n",
    "            sku_aisle_mapping[row['item_number']] = []\n",
    "        sku_aisle_mapping[row['item_number']].append(row['aisle_sequence'])\n",
    "    \n",
    "    # For each SKU, sort its aisles\n",
    "    for sku in sku_aisle_mapping:\n",
    "        sku_aisle_mapping[sku].sort()\n",
    "    \n",
    "    # Get all container IDs\n",
    "    container_ids = container_data['container_id'].unique().tolist()\n",
    "    \n",
    "    # Compute features for each container\n",
    "    feature_arrays = []\n",
    "    valid_containers = []\n",
    "    container_features = {}\n",
    "    \n",
    "    for container_id in container_ids:\n",
    "        # Compute container features\n",
    "        centroid, span, distinct_aisles = compute_container_features(container_id, container_data, sku_aisle_mapping)\n",
    "        container_features[container_id] = (centroid, span, distinct_aisles)\n",
    "        \n",
    "        # Skip containers with no aisle data\n",
    "        if centroid == 0 and span == 0:\n",
    "            continue\n",
    "            \n",
    "        # Choose secondary feature based on flag\n",
    "        secondary_feature = distinct_aisles if use_distinct_aisles else span\n",
    "        \n",
    "        feature_arrays.append([\n",
    "            centroid * centroid_weight,          # Aisle centroid \n",
    "            secondary_feature * secondary_weight # Secondary feature\n",
    "        ])\n",
    "        valid_containers.append(container_id)\n",
    "    \n",
    "    if not valid_containers:\n",
    "        logger.warning(\"No valid containers found with aisle data\")\n",
    "        return [], np.array([])\n",
    "    \n",
    "    # Create feature matrix\n",
    "    feature_matrix = np.array(feature_arrays)\n",
    "    \n",
    "    # Store a copy of the raw features before normalization (but after applying weights)\n",
    "    weighted_feature_matrix = feature_matrix.copy()\n",
    "    \n",
    "    # Create unweighted raw features by extracting the original values\n",
    "    raw_feature_matrix = np.zeros_like(feature_matrix)\n",
    "    raw_feature_matrix[:, 0] = feature_matrix[:, 0] / centroid_weight  # Remove centroid weight\n",
    "    raw_feature_matrix[:, 1] = feature_matrix[:, 1] / secondary_weight  # Remove secondary feature weight\n",
    "    \n",
    "    # Log feature statistics\n",
    "    logger.info(f\"Processed {len(valid_containers)} valid containers\")\n",
    "    logger.info(f\"Feature 1 (Centroid) - Mean: {np.mean(raw_feature_matrix[:, 0]):.2f}, \"\n",
    "               f\"Min: {np.min(raw_feature_matrix[:, 0]):.2f}, \"\n",
    "               f\"Max: {np.max(raw_feature_matrix[:, 0]):.2f}\")\n",
    "    logger.info(f\"Feature 2 ({('Distinct Aisles' if use_distinct_aisles else 'Aisle Span')}) - \"\n",
    "               f\"Mean: {np.mean(raw_feature_matrix[:, 1]):.2f}, \"\n",
    "               f\"Min: {np.min(raw_feature_matrix[:, 1]):.2f}, \"\n",
    "               f\"Max: {np.max(raw_feature_matrix[:, 1]):.2f}\")\n",
    "    \n",
    "    # Normalize weighted features\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(feature_matrix)\n",
    "    \n",
    "    return valid_containers, normalized_features, raw_feature_matrix\n",
    "\n",
    "def run_hdbscan_clustering(feature_matrix: np.ndarray, \n",
    "                          min_cluster_size: int = 5,\n",
    "                          min_samples: int = None, \n",
    "                          cluster_selection_epsilon: float = 0.0,\n",
    "                          alpha: float = 1.0,\n",
    "                          cluster_selection_method: str = 'eom') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run HDBSCAN clustering on the feature matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_matrix : np.ndarray\n",
    "        Matrix of normalized features for each container\n",
    "    min_cluster_size : int\n",
    "        Minimum number of containers in a cluster\n",
    "    min_samples : int\n",
    "        Number of samples in a neighborhood for a point to be a core point\n",
    "    cluster_selection_epsilon : float\n",
    "        Epsilon to use when determining cluster membership\n",
    "    alpha : float\n",
    "        Alpha to use in the RobustSingleLinkage\n",
    "    cluster_selection_method : str\n",
    "        Method used to select clusters ('eom' or 'leaf')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    labels : np.ndarray\n",
    "        Cluster labels for each container (-1 for noise)\n",
    "    \"\"\"\n",
    "    # Set min_samples to match min_cluster_size if not provided\n",
    "    if min_samples is None:\n",
    "        min_samples = min_cluster_size\n",
    "        \n",
    "    logger.info(f\"Running HDBSCAN with min_cluster_size={min_cluster_size}, \"\n",
    "               f\"min_samples={min_samples}, \"\n",
    "               f\"cluster_selection_epsilon={cluster_selection_epsilon}, \"\n",
    "               f\"alpha={alpha}, cluster_selection_method={cluster_selection_method}\")\n",
    "    \n",
    "    # Initialize and run HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        alpha=alpha,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        gen_min_span_tree=True\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    cluster_labels = clusterer.fit_predict(feature_matrix)\n",
    "    \n",
    "    # Count clusters\n",
    "    num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    noise_points = np.sum(cluster_labels == -1)\n",
    "    \n",
    "    logger.info(f\"HDBSCAN found {num_clusters} clusters and {noise_points} noise points\")\n",
    "    \n",
    "    return cluster_labels, clusterer\n",
    "\n",
    "def generate_visualizations(feature_matrix: np.ndarray, \n",
    "                           cluster_labels: np.ndarray,\n",
    "                           valid_containers: List[str],\n",
    "                           clusterer: hdbscan.HDBSCAN,\n",
    "                           output_dir: str,\n",
    "                           use_distinct_aisles: bool = True,\n",
    "                           raw_feature_matrix: np.ndarray = None) -> None:\n",
    "    \"\"\"\n",
    "    Generate visualizations for the clustering results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_matrix : np.ndarray\n",
    "        Matrix of normalized features for each container\n",
    "    cluster_labels : np.ndarray\n",
    "        Cluster labels for each container\n",
    "    valid_containers : List[str]\n",
    "        List of container IDs that were successfully processed\n",
    "    clusterer : hdbscan.HDBSCAN\n",
    "        HDBSCAN clusterer object\n",
    "    output_dir : str\n",
    "        Directory to save visualizations\n",
    "    use_distinct_aisles : bool\n",
    "        Whether distinct aisles (True) or aisle span (False) was used as second feature\n",
    "    raw_feature_matrix : np.ndarray, optional\n",
    "        Matrix of raw (unweighted, unnormalized) features for visualization\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a colormap for the clusters\n",
    "    num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    cmap = plt.cm.get_cmap('tab20', num_clusters + 1)  # +1 for noise points\n",
    "    \n",
    "    # 1. Scatter plot of clusters with normalized features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot noise points first (gray)\n",
    "    noise_mask = (cluster_labels == -1)\n",
    "    plt.scatter(\n",
    "        feature_matrix[noise_mask, 0], \n",
    "        feature_matrix[noise_mask, 1],\n",
    "        c='gray', marker='x', alpha=0.5, label='Noise'\n",
    "    )\n",
    "    \n",
    "    # Then plot each cluster with a different color\n",
    "    for i in range(num_clusters):\n",
    "        mask = (cluster_labels == i)\n",
    "        plt.scatter(\n",
    "            feature_matrix[mask, 0], \n",
    "            feature_matrix[mask, 1],\n",
    "            c=[cmap(i)], marker='o', alpha=0.7, label=f'Cluster {i}'\n",
    "        )\n",
    "    \n",
    "    plt.title('HDBSCAN Clustering Results (Normalized Features)')\n",
    "    plt.xlabel('Aisle Centroid (normalized)')\n",
    "    plt.ylabel('Distinct Aisles (normalized)' if use_distinct_aisles else 'Aisle Span (normalized)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'hdbscan_clusters_normalized.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Scatter plot with raw (unweighted, unnormalized) features\n",
    "    if raw_feature_matrix is not None:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot noise points first (gray)\n",
    "        plt.scatter(\n",
    "            raw_feature_matrix[noise_mask, 0], \n",
    "            raw_feature_matrix[noise_mask, 1],\n",
    "            c='gray', marker='x', alpha=0.5, label='Noise'\n",
    "        )\n",
    "        \n",
    "        # Then plot each cluster with a different color\n",
    "        for i in range(num_clusters):\n",
    "            mask = (cluster_labels == i)\n",
    "            plt.scatter(\n",
    "                raw_feature_matrix[mask, 0], \n",
    "                raw_feature_matrix[mask, 1],\n",
    "                c=[cmap(i)], marker='o', alpha=0.7, label=f'Cluster {i}'\n",
    "            )\n",
    "        \n",
    "        plt.title('HDBSCAN Clustering Results (Raw Features)')\n",
    "        plt.xlabel('Aisle Centroid (raw)')\n",
    "        plt.ylabel('Distinct Aisles (raw)' if use_distinct_aisles else 'Aisle Span (raw)')\n",
    "        #plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'hdbscan_clusters_raw.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Cluster size distribution\n",
    "    cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    if -1 in cluster_sizes.index:\n",
    "        cluster_sizes = cluster_sizes.drop(-1)  # Remove noise points\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_sizes.plot(kind='bar', color='skyblue')\n",
    "    plt.axhline(y=np.mean(cluster_sizes), color='r', linestyle='--', label='Average Size')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of Containers')\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'cluster_sizes.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. HDBSCAN condensed tree\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    clusterer.condensed_tree_.plot(select_clusters=True)\n",
    "    plt.title('HDBSCAN Condensed Tree')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'hdbscan_tree.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. HDBSCAN minimum spanning tree\n",
    "    if hasattr(clusterer, 'minimum_spanning_tree_'):\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        clusterer.minimum_spanning_tree_.plot()\n",
    "        plt.title('HDBSCAN Minimum Spanning Tree')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'hdbscan_mst.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    logger.info(f\"Generated visualizations in {output_dir}\")\n",
    "\n",
    "def create_output_dataframe(valid_containers: List[str], \n",
    "                           cluster_labels: np.ndarray,\n",
    "                           container_data: pd.DataFrame,\n",
    "                           output_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe with container IDs and their cluster assignments.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    valid_containers : List[str]\n",
    "        List of container IDs that were successfully processed\n",
    "    cluster_labels : np.ndarray\n",
    "        Cluster labels for each container\n",
    "    container_data : pd.DataFrame\n",
    "        Original container data\n",
    "    output_dir : str\n",
    "        Directory to save output file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_df : pd.DataFrame\n",
    "        DataFrame with container IDs and cluster assignments\n",
    "    \"\"\"\n",
    "    # Create dataframe with container IDs and cluster assignments\n",
    "    cluster_df = pd.DataFrame({\n",
    "        'container_id': valid_containers,\n",
    "        'cluster': cluster_labels\n",
    "    })\n",
    "    \n",
    "    # Map -1 to \"noise\"\n",
    "    cluster_df['cluster_name'] = cluster_df['cluster'].apply(\n",
    "        lambda x: f\"cluster_{x}\" if x >= 0 else \"noise\"\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(output_dir, 'container_clusters.csv')\n",
    "    cluster_df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"Saved container cluster assignments to {output_path}\")\n",
    "    \n",
    "    # Log cluster statistics\n",
    "    cluster_counts = cluster_df['cluster_name'].value_counts()\n",
    "    logger.info(\"Cluster statistics:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        logger.info(f\"  {cluster}: {count} containers\")\n",
    "    \n",
    "    return cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\i'\n",
      "C:\\Users\\abhatt\\AppData\\Local\\Temp\\ipykernel_51508\\3562566299.py:12: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  container_data = pd.read_csv('root\\input\\container_data.csv')\n",
      "C:\\Users\\abhatt\\AppData\\Local\\Temp\\ipykernel_51508\\3562566299.py:16: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  slotbook_data = pd.read_csv('root\\input\\slotbook_data.csv')\n",
      "2025-03-10 09:45:47,905 - hdbscan_clustering - INFO - Configuration loaded \n",
      "2025-03-10 09:45:48,000 - hdbscan_clustering - INFO - Preprocessing container data for clustering\n",
      "2025-03-10 09:45:51,036 - hdbscan_clustering - INFO - Processed 13530 valid containers\n",
      "2025-03-10 09:45:51,037 - hdbscan_clustering - INFO - Feature 1 (Centroid) - Mean: 27.78, Min: 1.00, Max: 72.00\n",
      "2025-03-10 09:45:51,038 - hdbscan_clustering - INFO - Feature 2 (Aisle Span) - Mean: 11.71, Min: 0.00, Max: 70.00\n",
      "2025-03-10 09:45:51,042 - hdbscan_clustering - INFO - Running HDBSCAN with min_cluster_size=80, min_samples=2, cluster_selection_epsilon=0.01, alpha=1.0, cluster_selection_method=eom\n",
      "c:\\Users\\abhatt\\OneDrive - Chewy.com, LLC\\Desktop\\Pick Planning\\fc-sci-pick-planning-model\\pickplan-env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhatt\\OneDrive - Chewy.com, LLC\\Desktop\\Pick Planning\\fc-sci-pick-planning-model\\pickplan-env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-10 09:45:51,196 - hdbscan_clustering - INFO - HDBSCAN found 57 clusters and 1805 noise points\n",
      "C:\\Users\\abhatt\\AppData\\Local\\Temp\\ipykernel_51508\\3557502812.py:286: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab20', num_clusters + 1)  # +1 for noise points\n",
      "c:\\Users\\abhatt\\OneDrive - Chewy.com, LLC\\Desktop\\Pick Planning\\fc-sci-pick-planning-model\\pickplan-env\\Lib\\site-packages\\hdbscan\\plots.py:837: RuntimeWarning: divide by zero encountered in divide\n",
      "  line_width = edge_linewidth * (np.log(self._mst.T[2].max() / self._mst.T[2]) + 1.0)\n",
      "c:\\Users\\abhatt\\OneDrive - Chewy.com, LLC\\Desktop\\Pick Planning\\fc-sci-pick-planning-model\\pickplan-env\\Lib\\site-packages\\matplotlib\\lines.py:78: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  scaled_offset = offset * lw\n",
      "2025-03-10 09:45:55,310 - hdbscan_clustering - INFO - Generated visualizations in output/hdbscan_results\n",
      "2025-03-10 09:45:55,332 - hdbscan_clustering - INFO - Saved container cluster assignments to output/hdbscan_results\\container_clusters.csv\n",
      "2025-03-10 09:45:55,334 - hdbscan_clustering - INFO - Cluster statistics:\n",
      "2025-03-10 09:45:55,335 - hdbscan_clustering - INFO -   noise: 1805 containers\n",
      "2025-03-10 09:45:55,336 - hdbscan_clustering - INFO -   cluster_4: 863 containers\n",
      "2025-03-10 09:45:55,336 - hdbscan_clustering - INFO -   cluster_8: 455 containers\n",
      "2025-03-10 09:45:55,336 - hdbscan_clustering - INFO -   cluster_1: 418 containers\n",
      "2025-03-10 09:45:55,337 - hdbscan_clustering - INFO -   cluster_6: 401 containers\n",
      "2025-03-10 09:45:55,337 - hdbscan_clustering - INFO -   cluster_40: 373 containers\n",
      "2025-03-10 09:45:55,338 - hdbscan_clustering - INFO -   cluster_18: 341 containers\n",
      "2025-03-10 09:45:55,338 - hdbscan_clustering - INFO -   cluster_19: 330 containers\n",
      "2025-03-10 09:45:55,339 - hdbscan_clustering - INFO -   cluster_43: 330 containers\n",
      "2025-03-10 09:45:55,340 - hdbscan_clustering - INFO -   cluster_48: 326 containers\n",
      "2025-03-10 09:45:55,340 - hdbscan_clustering - INFO -   cluster_21: 324 containers\n",
      "2025-03-10 09:45:55,341 - hdbscan_clustering - INFO -   cluster_26: 311 containers\n",
      "2025-03-10 09:45:55,341 - hdbscan_clustering - INFO -   cluster_3: 298 containers\n",
      "2025-03-10 09:45:55,341 - hdbscan_clustering - INFO -   cluster_39: 273 containers\n",
      "2025-03-10 09:45:55,342 - hdbscan_clustering - INFO -   cluster_10: 254 containers\n",
      "2025-03-10 09:45:55,342 - hdbscan_clustering - INFO -   cluster_32: 240 containers\n",
      "2025-03-10 09:45:55,343 - hdbscan_clustering - INFO -   cluster_47: 236 containers\n",
      "2025-03-10 09:45:55,343 - hdbscan_clustering - INFO -   cluster_46: 235 containers\n",
      "2025-03-10 09:45:55,344 - hdbscan_clustering - INFO -   cluster_35: 213 containers\n",
      "2025-03-10 09:45:55,344 - hdbscan_clustering - INFO -   cluster_56: 213 containers\n",
      "2025-03-10 09:45:55,345 - hdbscan_clustering - INFO -   cluster_51: 210 containers\n",
      "2025-03-10 09:45:55,345 - hdbscan_clustering - INFO -   cluster_14: 207 containers\n",
      "2025-03-10 09:45:55,346 - hdbscan_clustering - INFO -   cluster_41: 201 containers\n",
      "2025-03-10 09:45:55,346 - hdbscan_clustering - INFO -   cluster_38: 200 containers\n",
      "2025-03-10 09:45:55,347 - hdbscan_clustering - INFO -   cluster_24: 199 containers\n",
      "2025-03-10 09:45:55,347 - hdbscan_clustering - INFO -   cluster_36: 191 containers\n",
      "2025-03-10 09:45:55,348 - hdbscan_clustering - INFO -   cluster_34: 190 containers\n",
      "2025-03-10 09:45:55,348 - hdbscan_clustering - INFO -   cluster_52: 189 containers\n",
      "2025-03-10 09:45:55,348 - hdbscan_clustering - INFO -   cluster_37: 186 containers\n",
      "2025-03-10 09:45:55,349 - hdbscan_clustering - INFO -   cluster_55: 167 containers\n",
      "2025-03-10 09:45:55,349 - hdbscan_clustering - INFO -   cluster_2: 162 containers\n",
      "2025-03-10 09:45:55,350 - hdbscan_clustering - INFO -   cluster_27: 155 containers\n",
      "2025-03-10 09:45:55,350 - hdbscan_clustering - INFO -   cluster_29: 153 containers\n",
      "2025-03-10 09:45:55,355 - hdbscan_clustering - INFO -   cluster_42: 151 containers\n",
      "2025-03-10 09:45:55,365 - hdbscan_clustering - INFO -   cluster_53: 147 containers\n",
      "2025-03-10 09:45:55,368 - hdbscan_clustering - INFO -   cluster_30: 146 containers\n",
      "2025-03-10 09:45:55,370 - hdbscan_clustering - INFO -   cluster_22: 140 containers\n",
      "2025-03-10 09:45:55,371 - hdbscan_clustering - INFO -   cluster_54: 135 containers\n",
      "2025-03-10 09:45:55,372 - hdbscan_clustering - INFO -   cluster_44: 134 containers\n",
      "2025-03-10 09:45:55,372 - hdbscan_clustering - INFO -   cluster_31: 131 containers\n",
      "2025-03-10 09:45:55,375 - hdbscan_clustering - INFO -   cluster_9: 126 containers\n",
      "2025-03-10 09:45:55,376 - hdbscan_clustering - INFO -   cluster_13: 126 containers\n",
      "2025-03-10 09:45:55,377 - hdbscan_clustering - INFO -   cluster_45: 122 containers\n",
      "2025-03-10 09:45:55,378 - hdbscan_clustering - INFO -   cluster_12: 119 containers\n",
      "2025-03-10 09:45:55,379 - hdbscan_clustering - INFO -   cluster_50: 117 containers\n",
      "2025-03-10 09:45:55,380 - hdbscan_clustering - INFO -   cluster_5: 114 containers\n",
      "2025-03-10 09:45:55,381 - hdbscan_clustering - INFO -   cluster_7: 113 containers\n",
      "2025-03-10 09:45:55,381 - hdbscan_clustering - INFO -   cluster_15: 110 containers\n",
      "2025-03-10 09:45:55,382 - hdbscan_clustering - INFO -   cluster_16: 106 containers\n",
      "2025-03-10 09:45:55,383 - hdbscan_clustering - INFO -   cluster_33: 103 containers\n",
      "2025-03-10 09:45:55,384 - hdbscan_clustering - INFO -   cluster_25: 99 containers\n",
      "2025-03-10 09:45:55,384 - hdbscan_clustering - INFO -   cluster_0: 98 containers\n",
      "2025-03-10 09:45:55,385 - hdbscan_clustering - INFO -   cluster_20: 95 containers\n",
      "2025-03-10 09:45:55,385 - hdbscan_clustering - INFO -   cluster_23: 94 containers\n",
      "2025-03-10 09:45:55,386 - hdbscan_clustering - INFO -   cluster_11: 92 containers\n",
      "2025-03-10 09:45:55,386 - hdbscan_clustering - INFO -   cluster_28: 88 containers\n",
      "2025-03-10 09:45:55,387 - hdbscan_clustering - INFO -   cluster_49: 88 containers\n",
      "2025-03-10 09:45:55,387 - hdbscan_clustering - INFO -   cluster_17: 87 containers\n",
      "2025-03-10 09:45:55,388 - hdbscan_clustering - INFO - HDBSCAN clustering completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Load and validate configuration\n",
    "config = load_config('root/config/hdbscan_config.yaml')\n",
    "\n",
    "# Print configuration\n",
    "logger.info(f\"Configuration loaded \")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = config['output']['dir']\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "container_data = pd.read_csv('root\\input\\container_data.csv')\n",
    "sampled_containers = container_data['container_id'].unique()\n",
    "container_data = container_data[container_data['container_id'].isin(sampled_containers)]\n",
    "\n",
    "slotbook_data = pd.read_csv('root\\input\\slotbook_data.csv')\n",
    "\n",
    " # Basic data validation\n",
    "required_container_cols = ['container_id', 'item_number']\n",
    "required_slotbook_cols = ['item_number', 'aisle_sequence']\n",
    "\n",
    "for col in required_container_cols:\n",
    "    if col not in container_data.columns:\n",
    "        raise ValueError(f\"Container data missing required column: {col}\")\n",
    "\n",
    "for col in required_slotbook_cols:\n",
    "    if col not in slotbook_data.columns:\n",
    "        raise ValueError(f\"Slotbook data missing required column: {col}\")\n",
    "\n",
    "# Preprocess data\n",
    "use_distinct_aisles = config['features']['use_distinct_aisles']\n",
    "centroid_weight = config['features']['centroid_weight']\n",
    "secondary_weight = config['features']['secondary_weight']\n",
    "\n",
    "valid_containers, feature_matrix, raw_feature_matrix = preprocess_data(\n",
    "    container_data, \n",
    "    slotbook_data,\n",
    "    use_distinct_aisles=use_distinct_aisles,\n",
    "    centroid_weight=centroid_weight,\n",
    "    secondary_weight=secondary_weight\n",
    ")\n",
    "\n",
    "if len(valid_containers) == 0:\n",
    "    logger.error(\"No valid containers found. Exiting.\")\n",
    "\n",
    "\n",
    "# Run HDBSCAN clustering\n",
    "hdbscan_config = config['hdbscan']\n",
    "cluster_labels, clusterer = run_hdbscan_clustering(\n",
    "    feature_matrix,\n",
    "    min_cluster_size=hdbscan_config['min_cluster_size'],\n",
    "    min_samples=hdbscan_config['min_samples'],\n",
    "    cluster_selection_epsilon=hdbscan_config['cluster_selection_epsilon'],\n",
    "    alpha=hdbscan_config['alpha'],\n",
    "    cluster_selection_method=hdbscan_config['cluster_selection_method']\n",
    ")\n",
    "\n",
    "# Generate visualizations\n",
    "if config['visualizations']['enabled']:\n",
    "    generate_visualizations(\n",
    "        feature_matrix,\n",
    "        cluster_labels,\n",
    "        valid_containers,\n",
    "        clusterer,\n",
    "        output_dir,\n",
    "        use_distinct_aisles=use_distinct_aisles,\n",
    "        raw_feature_matrix=raw_feature_matrix\n",
    "    )\n",
    "\n",
    "# Create output dataframe\n",
    "cluster_df = create_output_dataframe(\n",
    "    valid_containers,\n",
    "    cluster_labels,\n",
    "    container_data,\n",
    "    output_dir\n",
    ")\n",
    "\n",
    "logger.info(\"HDBSCAN clustering completed successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pickplan-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
